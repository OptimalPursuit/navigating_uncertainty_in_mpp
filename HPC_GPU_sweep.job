#!/bin/bash
#SBATCH --job-name=torchrl_main               # Job name
#SBATCH --output=output_files/gpu_job.%j.out   # Slurm job output
#SBATCH --error=output_files/gpu_job.%j.err    # Slurm job error log
#SBATCH --cpus-per-task=32                     # CPUs per task
#SBATCH --gres=gpu:1                           # GPU request
#SBATCH --time=3-00:00:00                      # Job run time (d-hh:mm:ss)
#SBATCH --mail-type=FAIL,END                   # Email notification
#SBATCH --exclude=cn[1-4]                      # Exclude specific nodes
#SBATCH --exclude=cn[1-4]
#SBATCH --exclude=cn[6-19]
#SBATCH --exclude=desktop[1-16]
#SBATCH --nodelist=cn[5]
#SBATCH --partition=pcn345

# Ensure the output directory exists
mkdir -p output_files

# Print the hostname of the allocated node
hostname

# Define the path to the Python script you want to run
SCRIPT_PATH="sweep.py"

# Run the Python script with log_file
python3 "$SCRIPT_PATH" "$@"