algorithm:
  clip_range: 0.2
  demand_lambda: 1.0
  entropy_lambda: 0.010071090711145904
  feasibility_lambda: 1.0
  gae_lambda: 0.95
  gamma: 0.99
  lagrangian_multiplier_0: 0.7234141894161281
  lagrangian_multiplier_1: 0.07759646366807803
  lagrangian_multiplier_10: 0.040925127350765034
  lagrangian_multiplier_11: 5.773866244644313
  lagrangian_multiplier_12: 0.011063485477038284
  lagrangian_multiplier_13: 0.3948884327817033
  lagrangian_multiplier_14: 0.818033314922533
  lagrangian_multiplier_15: 0.3796017361801996
  lagrangian_multiplier_16: 1.157046673493113
  lagrangian_multiplier_17: 0.3093313596381579
  lagrangian_multiplier_18: 0.11827692212755656
  lagrangian_multiplier_19: 0.6784230089774258
  lagrangian_multiplier_2: 4.597319656332837
  lagrangian_multiplier_20: 5.943307314826245
  lagrangian_multiplier_21: 1.084098825482031
  lagrangian_multiplier_22: 0.20397107967630015
  lagrangian_multiplier_23: 0.86017601823972
  lagrangian_multiplier_24: 0.03923812379807979
  lagrangian_multiplier_3: 0.43143535108674297
  lagrangian_multiplier_4: 6.185582805183226
  lagrangian_multiplier_5: 0.4820437519538035
  lagrangian_multiplier_6: 4.529370922722404
  lagrangian_multiplier_7: 3.1172209774478494
  lagrangian_multiplier_8: 0.06873276769273486
  lagrangian_multiplier_9: 0.02634882940288257
  max_grad_norm: 0.5
  mini_batch_size: 0.5
  ppo_epochs: 5
  primal_dual: false
  stability_lambda: 1.0
  tau: 0.005
  type: ppo
  vf_lambda: 0.5
env:
  CI_target: 1.25
  LCG_target: 0.95
  TEU: 1000
  VCG_target: 1.05
  bays: 10
  block_stowage_mask: false
  blocks: 1
  capacity:
  - 50
  cargo_classes: 6
  customer_classes: 2
  cv_demand: 0.5
  decks: 2
  demand_uncertainty: true
  env_name: mpp
  episode_order: standard
  generalization: false
  hatch_overstowage_costs: 0.333333
  hatch_overstowage_mask: false
  iid_demand: true
  limit_revenue: true
  long_crane_costs: 0.5
  normalize_obs: true
  perturbation: 0.2
  ports: 4
  seed: 42
  spot_percentage: 0.3
  stability_difference: 0.1
  utilization_rate_initial_demand: 1.1
  weight_classes: 3
model:
  batch_size: 64
  critic_temperature: 1.0
  decoder_type: attention
  dropout_rate: 0.008972135903337364
  dyn_embed: self_attention
  embed_dim: 128
  encoder_type: attention
  hidden_dim: 512
  init_dim: 8
  logger: wandb
  lr_end_factor: 0.5
  normalization: layer
  num_decoder_layers: 4
  num_encoder_layers: 3
  num_heads: 8
  phase: train
  scale_max: 1.931286785557626
  tanh_clipping: 0
  tanh_squashing: false
  temperature: 0.11243639449117128
testing:
  feasibility_recovery: false
  folder: ppo-fr
  num_episodes: 30
  path: results/trained_models/navigating_uncertainty
training:
  lr: 0.00014690714579803494
  optimizer: Adam
  pd_lr: 3.469071457980349e-05
  projection_kwargs:
    alpha: 0.01
    delta: 0.01
    max_iter: 300
    n_action: 20
    n_constraints: 25
    scale: 0.00025463040788043916
    slack_penalty: 10000
    use_early_stopping: true
  projection_type: None
  scale_max: 9.46
  test_data_size: 5000
  train_data_size: 7200000
  val_data_size: 5000
  validation_freq: 0.2
  validation_patience: 2
wandb_version: 1
